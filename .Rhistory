counter <- 1 #and a counter
for(i in 1:nperm){
utils::setTxtProgressBar(pb,counter) #update progress
counter <- counter+1 #update counter
samp_order <- sample(nrow(response))
r.p <- response[samp_order,samp_order]
yp <- r.p[lower.tri(r.p)]
if(!is.null(offset)){
offset.p <- offset[samp_order,samp_order]
op <- offset.p[lower.tri(offset.p)]
}else{
op <- NULL
}
if(!is.null(weights)){
weights.p <- weights[samp_order,samp_order]
wp <- weights.p[lower.tri(weights.p)]
}else{
wp <- NULL
}
if(family != "betar" & family != "negbin"){
z.perm[i,] <- summary(stats::glm(yp ~ x, family = family, offset = op, weights = wp))$coefficients[-1,3]
}
if(family == "betar"){
z.perm[i,] <- summary(betareg::betareg(yp ~ x, weights = wp))$coefficients$mean[-1,3]
}
if(family == "negbin"){
z.perm[i,] <- summary(MASS::glm.nb(yp ~ x + offset(op), weights = wp))$coefficients[-1,3]
}
}
}
pval <- sapply(1:length(predictors), function(p){
min(
c(sum(z.perm[,p] >= z.val[p])+1,
sum(z.perm[,p] <= z.val[p])+1
))/(nperm+1)
})*2
res <- list(
call = fm,
pred_names = x_names,
family = family,
permutation = permutation,
nperm = nperm,
coefficients = ifelse(family == "betar", mod.orig$coefficients$mean[-1], mod.orig$coefficients[-1]),
stderr = ifelse(family == "betar", summary(mod.orig)$coefficients$mean[-1,2], summary(mod.orig)$coefficients[-1,2]),
z = z.val,
p = pval,
permuted_z = z.perm,
aic = AIC(mod.orig),
bic = BIC(mod.orig),
loglik = as.numeric(logLik(mod.orig)),
weights = deparse(substitute(weights)),
offset = deparse(substitute(offset))
)
if(family == "betar"){
res$precision <- mod.orig$coefficients$precision
}
if(family == "negbin"){
res$theta <- mod.orig$theta
}
class(res) <- "glmqap"
return(res)
}
m1 <- glmqap(y ~ x1 + x2)
print(m1)
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n\n"))
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
print(m1)
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n\n"))
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("\n")
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
print(m1)
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n\n"))
cat(paste("Weights:", x$weights), "\n\n")
cat(paste("Offset:", x$offset), "\n\n")
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("\n")
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
print(m1)
m1
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n"))
cat(paste("Weights:", x$weights), "\n")
cat(paste("Offset:", x$offset), "\n")
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("\n")
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
m1
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n"))
cat(paste("Weights:", x$weights), "\n")
cat(paste("Offset:", x$offset), "\n\n")
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("\n")
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
m1
AIC(m1)
m1 <- glmqap(y ~ x1 + x2, permutation = "Y")
m1
m1 <- glmqap(y ~ x1 + x2, permutation = "X")
m1
m1 <- glmqap(y ~ x1 + x2, permutation = "X", weights = matrix(runif(100,0,1),10,10))
m1
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n"))
cat(paste("Weights:", x$weights), "\n")
cat(paste("Offset:", x$offset), "\n")
cat(paste("Permutations: ", x$nperm), "\n\n")
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("\n")
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
m1
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n"))
cat(paste("Family: ", x$family), "\n")
cat(paste("Weights:", x$weights), "\n")
cat(paste("Offset:", x$offset), "\n")
cat(paste("Permutations: ", x$nperm), "\n\n")
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("\n")
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
m1
library(aninet)
library(aninet)
?print.glmqap
library(aninet)
?aninet
.libPaths()
.libPaths(.libPaths()[2])
library(aninet)
print.glmqap
y
y <- matrix(runif(100,0,1), nrow = 10, ncol = 10)
x1 <- matrix(runif(100,0,1), nrow = 10, ncol = 10)
x2 <- matrix(runif(100,0,1), nrow = 10, ncol = 10)
m1 <- glmqap(y ~ x1 + x2)
m1
library(aninet)
m1 <- glmqap(y ~ x1 + x2)
m1
print(m1)
?print.glmqap
print(m1)
m1
#' Print function of glmqap objects
#'
#' Prints summary information about GLMQAP models
#'
#' @param x A glmqap object
#' @details Prints formatted results from GLMQAP fits, including formula, family, permutations, weights and offsets, coefficients, standard errors, test statistics, and p-values
#' @return Printed results
print.glmqap <- function(x){
perm_method <- ifelse(x$permutation == "X", "X permutation", ifelse(x$permutation == "Y", "Y permutation", "Doube-Semi-Partialling"))
cat(paste("GLMQAP with ", perm_method, "\n\n", sep = ""))
cat(paste("Formula: ", Reduce(paste, deparse(x$call)), "\n"))
cat(paste("Family: ", x$family), "\n")
cat(paste("Weights:", x$weights), "\n")
cat(paste("Offset:", x$offset), "\n")
cat(paste("Permutations: ", x$nperm), "\n\n")
cat("Coefficients:\n")
results_mat <- cbind(x$coefficients, x$stderr, x$z, x$p)
colnames(results_mat) <- c("Estimate", "Std. Error", "Z", "P(two-tailed)")
row.names(results_mat) <- x$pred_names
print.table(results_mat)
cat("\n")
cat("log-likelihood:", x$loglik, "\t")
cat("AIC:", x$aic, "\t")
cat("BIC:", x$bic, "\t")
cat("\n")
}
m1
class(m1)
library(aninet)
m1 <- glmqap(y ~ x1 + x2)
print(m1)
m1
y <- matrix(runif(100,0,1), nrow = 10, ncol = 10)
x1 <- matrix(runif(100,0,1), nrow = 10, ncol = 10)
x2 <- matrix(runif(100,0,1), nrow = 10, ncol = 10)
m1 <- glmqap(y ~ x1 + x2)
print(m1)
print.glmqap(m1)
library(aninet)
m1 <- glmqap(y ~ x1 + x2)
m1
gai(y ~ x1 + x2)
gai(y ~ x1 + x2, type = "deviance")
gai(y ~ x1 + x2, type = "deviance")
gai(y ~ x1 + x2)
library(aninet)
gai(y ~ x1 + x2)
joint_gregariousness(y)
joint_gregariousness(y)
hist(joint_gregariousness(y))
d <- sample(10,100,rep=T)
x <- rbinom(size = d, n = 100, prob = 0.1)
hist(x/d)
social_differentiation(x,d)
social_differentiation(x,d,method="Whitehead")
library(aninet)
file.exists("~/.ssh/id_rsa.pub")
curve(x^2)
20*20
x <- NA
x <- NA
x[1] <- 1
while(x < 8000000000){
x[(length(x)+1)] <- 2*x
}
warnings()
length(x)+1
x
x[1] <- 1
x <- NA
x[1] <- 1
while(x < 8000000000){
print(x)
x[(length(x)+1)] <- 2*x[length(x)]
}
while(x[length(x)] < 8000000000){
print(x)
x[(length(x)+1)] <- 2*x[length(x)]
}
x
plot(x)
x <- NA
x[1] <- 1
while(x[length(x)] < 8000000000){
print(x)
x[(length(x)+1)] <- 2*x[length(x)]
}
x
length(x)
plot(x)
34*6
204/3
204/30
min(which(x >= 25000))
min(which(x >= 250000))
19*6
114/30
day <- 1:length(x)*6 - 6
day
x[1]
x[2]
x[3]
198/30
19*6
198-114
84/30
84/30.5
devtools::install_github("https://github.com/MNWeiss/aninet")
.libPaths()
.libPaths(.libPaths()[2])
devtools::install_github("https://github.com/MNWeiss/aninet")
x <- 1
while(x[length(x)] < 8000000000) x[(length(x) + 1)] <- x[length(x)]*2
x
plot(x)
min(which(x >= 300000))
20*6
6*length(x)
10*6
.libPaths(.libPaths()[2])
document()
devtools::document()
?hclust
?as.dist
?cophenetic
?cor
?cutree
setwd("C:/Users/mw607/OneDrive - University of Exeter/PhD Chapters/Drone observation") #change to the directory where you've saved the data
require(MASS)
require(tnet)
require(betareg)
require(asnipe)
# Import interaction data ####
events <- read.csv("Data/Aggregated events.csv")
events$Subject <- as.character(events$Subject)
events$Modifiers <- as.character(events$Modifiers)
events$Behavior <- as.character(events$Behavior)
# Get individual attribute and kinship data ####
attributes <- read.csv("Data/attribute data.csv")
id <- as.character(attributes$id)
asc <- as.character(attributes$asc)
matriline <- as.character(attributes$matriline)
kin <- as.matrix(read.csv("Data/kinship.csv", row.names = 1))
# Get effort and interaction counts ####
#make the observation ID column a character
events$Observation.id <- as.character(events$Observation.id)
#video IDs
obs <- unique(events$Observation.id)
#total observation time per individual
ind_time <- sapply(id,function(x) sum(events$Duration..s.[events$Subject==x & events$Behavior=="visible"]))
#arrays to hold effort and interaction counts in each video
effort <- surfacing <- contact <- array(dim = c(length(obs),length(id),length(id)))
for(i in 1:length(obs)){
print(i/length(obs))
for(j in 1:length(id)){
for(k in 1:length(id)){
start.j <- events$Start..s.[events$Subject == id[j] & events$Observation.id == obs[i]] #the start of all visible states for j in i
stop.j <- events$Stop..s.[events$Subject == id[j] & events$Observation.id == obs[i]] #the end of all visible spells
total.j <- sum(stop.j - start.j) #total time in video
start.k <- events$Start..s.[events$Subject == id[k] & events$Observation.id == obs[i]] #same for k
stop.k <- events$Stop..s.[events$Subject == id[k] & events$Observation.id == obs[i]]
total.k <- sum(stop.k - start.k)
if(sum(total.j) > 0 & sum(total.k) > 0){ #if both were seen in the video
int_overlap <- 0 #start an overlap counter
for(x in 1:length(start.k)){ #for each k start
start.kx <- start.k[x]
stop.kx <- stop.k[x]
for(y in 1:length(start.j)){ #for each j start
start.jy <- start.j[y]
stop.jy <- stop.j[y]
if((start.kx <= start.jy & start.jy < stop.kx) | (start.jy <= start.kx & start.kx < stop.jy)){
max.start <- max(c(start.kx,start.jy))
min.end <- min(c(stop.kx,stop.jy))
int_overlap <- int_overlap+(min.end-max.start)
}
}
}
if(i == 1){
effort[i,j,k] <- total.j + total.k - int_overlap
}else{
effort[i,j,k] <- effort[(i-1),j,k] +  total.j + total.k - int_overlap
}
}else{
if(i == 1){
effort[i,j,k] <- total.j + total.k
}else{
effort[i,j,k] <- effort[(i-1),j,k] +  total.j + total.k
}
}
}
}
for(j in 1:length(id)){
for(k in 1:length(id)){
contact[i,j,k] <- sum(events$Behavior == "contact" & events$Subject == id[j] & events$Modifiers == id[k] & events$Observation.id == obs[i])
surfacing[i,j,k] <- sum(events$Behavior == "sync. surfacing" & events$Subject == id[j] & events$Modifiers == id[k] & events$Observation.id == obs[i])
}
}
}
# Check data ####
all_surf <- all_cont <- list() # a list to hold all interactions
surf.counter <- cont.counter <- 1
#Because each individual was sequentially "followed" in each video, every interaction is recorded twice, once for each individual
#Because of this, there are slight differences (usually less than 1 second) in the time of each event's two recordings.
#We take the midpoint between each recording of the event as the time of occurence, for simplicity
for(i in 1:(length(id)-1)){ #for each individual (except the last one)
for(j in i:length(id)){ #for all individuals after
for(k in 1:length(obs)){ #for each observations
if(any(events$Subject == id[i] & events$Modifiers == id[j] & events$Observation.id == obs[k])){ #if there were any interactions between this dyad in that observation
obs.ijk <- events[events$Observation.id == obs[k] & ((events$Subject == id[i]&events$Modifiers==id[j])|(events$Subject == id[j]&events$Modifiers==id[i]) ),] #get those events
if(any(obs.ijk$Behavior == "sync. surfacing")){
surf.ijk <- sort(obs.ijk$Start..s.[obs.ijk$Behavior == "sync. surfacing"])
n_surf <- length(surf.ijk)/2
surf_time <- rep(NA,n_surf)
for(s in 1:n_surf){
surf_time[s] <- mean(surf.ijk[(s*2-1):(s*2)]) #since events are recorded twice, take the midway point between each event as the "true" time
}
all_surf[[surf.counter]] <- data.frame(dyad = paste(sort(c(id[i],id[j])),collapse="-"), observation = obs[k], time = surf_time) #save
surf.counter <- surf.counter+1
}
#same thing for contacts
if(any(obs.ijk$Behavior == "contact")){
cont.ijk <- sort(obs.ijk$Start..s.[obs.ijk$Behavior == "contact"])
n_cont <- length(cont.ijk)/2
cont_time <- rep(NA,n_cont)
for(c in 1:n_cont){
cont_time[c] <- mean(cont.ijk[(c*2-1):(c*2)])
}
all_cont[[cont.counter]] <- data.frame(dyad = paste(sort(c(id[i],id[j])),collapse="-"), observation = obs[k], time = cont_time)
cont.counter <- cont.counter+1
}
}
}
}
}
all_surf <- do.call(rbind,all_surf) #turn them into dataframes
all_cont <- do.call(rbind,all_cont)
possible_inds_surf <- possible_inds_cont <- list() #lists to hold possible individuals
#get individuals present during each surfacing interaction
for(i in 1:nrow(all_surf)){
obs.i <- all_surf$observation[i]
time.i <- all_surf$time[i]
possible_inds_surf[[i]] <- id[sapply(id, function(x) any(events$Behavior == "visible" & events$Start..s. <= time.i & events$Stop..s. >= time.i & events$Observation.id == obs.i & events$Subject == x) )] #the set of individuals visible when the interaction occured
}
#same for contacts
for(i in 1:nrow(all_cont)){
obs.i <- all_cont$observation[i]
time.i <- all_cont$time[i]
possible_inds_cont[[i]] <- id[sapply(id, function(x) any(events$Behavior == "visible" & events$Start..s. <= time.i & events$Stop..s. >= time.i & events$Observation.id == obs.i & events$Subject == x) )]
}
surf_ints <- do.call(rbind,strsplit(as.character(all_surf$dyad),split="-"))
cont_ints <- do.call(rbind,strsplit(as.character(all_cont$dyad),split="-"))
#All interactions symmetric in each video?
#Lack of symmetry would indicate some interactions were only recorded for one member of a dyad, which would be bad
all(apply(surfacing,1,isSymmetric))
all(apply(contact,1,isSymmetric))
#All individuals marked as visible during their observed interactions?
#If either were false, either some individuals were incorrectly coded as not visible, or were incorrectly coded as interacting
all(sapply(1:nrow(surf_ints), function(z) all(surf_ints[z,] %in% possible_inds_surf[[z]]) ))
all(sapply(1:nrow(cont_ints), function(z) all(cont_ints[z,] %in% possible_inds_cont[[z]]) ))
# Get interaction rates and rates of co-detection #####
# interaction rates ####
#total interactions and effort
contact_tot <- apply(contact,c(2,3),sum)
surfacing_tot <- apply(surfacing,c(2,3),sum)
effort_tot <- effort[dim(effort)[1],,]
diag(effort_tot) <- ind_time
#add column and row names
row.names(effort_tot) <- colnames(effort_tot) <- colnames(contact_tot) <- colnames(surfacing_tot) <- row.names(contact_tot) <- row.names(surfacing_tot) <- id
#divide interactions by effort to get rates (per second here)
contact_rate <- contact_tot/effort_tot
surfacing_rate <- surfacing_tot/effort_tot
# temporal overlap ####
co_occur <- effort_tot
#since total effort is ind_time[i] + ind_time[j] - co_occur[i,j], we can get the co-occurerence time by substracting
#total effort from the sum of each individual's total time
for(i in 1:length(id)){
for(j in 1:length(id)){
co_occur[i,j] <- ind_time[i] + ind_time[j] - effort_tot[i,j]
}
}
co_occur[co_occur < 0] <- 0 #there are a few dyads were we end up with values less than zero (~ -3e-12). These have been checked and I'm fairly sure are the result of small rounding errors in R
overlap_portion <- co_occur/effort_tot #portion of time co-occurring
diag(overlap_portion) <- 0 #no self-overlap
weighted.sd <- function(x,w){
wm <- weighted.mean(x,w)
m <- sum(w > 0)
sqrt( sum(w*(x-wm)^2)/(((m-1)/m)*sum(w)) )
}
weighted.sd(overlap_portion[lower.tri(overlap_portion)], effort_tot[lower.tri(effort_tot)])
weighted.mean(overlap_portion[lower.tri(overlap_portion)], effort_tot[lower.tri(effort_tot)])
weighted.mean(overlap_portion[lower.tri(overlap_portion)], effort_tot[lower.tri(effort_tot)])/weighted.sd(overlap_portion[lower.tri(overlap_portion)], effort_tot[lower.tri(effort_tot)])
mean(overlap_portion[lower.tri(overlap_portion)], effort_tot[lower.tri(effort_tot)])/sd(overlap_portion[lower.tri(overlap_portion)], effort_tot[lower.tri(effort_tot)])
mean(overlap_portion[lower.tri(overlap_portion)])/sd(overlap_portion[lower.tri(overlap_portion)])
1.1300893/1.408864
